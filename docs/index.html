<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>
<title>AdaCurv</title>

<meta charset="utf-8">
<script src="js/template.v1.js"></script>
<!-- <script src="https://tpbarron.github.io/js/template.v1.js"></script> -->

<dt-article>
  <h1>AdaCurv</h1>
  <h2>Adaptive Curvature for Stochastic Optimization</h2>

  <hr>
  <h2>Getting Started</h2>
  <p>Clone the repository and add the repository to your python path:</p>

  <dt-code block language="bash">
    git clone https://github.com/tpbarron/adacurv.git
    export PYTHONPATH=$PYTHONPATH:/path/to/adacurv/
  </dt-code>

  <p>Install the dependencies:</p>
  <dt-code block language="bash">
    pip install -r requirements.txt
  </dt-code>

  <p>At this point one may run the samples in the experiments folder.</p>

  <hr>
  <h2>Project Structure</h2>

  <dt-code block language="html">
    adacurv/
      experiments/
        matrix_completion/
        mjrl/
        mnist/
        ...
      adacurv/
        torch/  - PyTorch implementation of AdaCurv
          optim/  - optimizer implementations
          utils/  - utilities for CG, Lanzcos, linesearch
        tf/     - Tensorflow implementation in progress
  </dt-code>

  <hr>
  <h2>Using an AdaCurv Optimizer</h2>

  <p>The AdaCurv optimizers depend on a the ability to compute the derivative of the loss repeatedly
     (for each matrix-vector product) and thus require a loss closure to be passed. The framework
     provides several common, preimplemented loss closures in `adacurv/torch/optim/hvp_closures.py`.
     For example, the MNIST experiment creates a `kl_closure` at each optimization step that depends
     on the model, input and output data, and a loss function (in this case `mean_kl_multinomial`
     because the Fisher is the Hessian of the KL divergence). This closure is then passed to the
     `optimizer.step` function.
   </p>

  <dt-code block language="python">
    closure = kl_closure(model, data, target, mean_kl_multinomial)
    optimizer.step(closure)
  </dt-code>

  <p>The optimizer takes care of the rest of the optimization including the curvature-vector products
    and adaptive parameter updates behind the scenes.</p>

  <hr>
  <p>Template adapted from <a href="https://distill.pub">Distill</a> with much gratitude.</p>

</dt-article>
